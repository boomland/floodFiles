{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "hw1_Pudyakov_Yaroslav.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PAVlLp-shicW",
        "sdffMT03hicX",
        "_pOO1DRQhice",
        "ERlLSC_ehicg",
        "WG2lchInhich",
        "IE0sq__Shicj",
        "-4vnlnVJhicj",
        "IzAiSy7ghick",
        "GJn-vyClhicl",
        "l8u2pT2bhicl",
        "oqc8wMzxhicm",
        "infExH7Yhicq",
        "Vte2peM-hicq",
        "FBwMD_5Chics",
        "4cN5j7cUhict",
        "_xXp3Qk7hict",
        "5IGqAoCHhicu",
        "Yk-_bcsghicv",
        "RDpnV62yhicv",
        "ll23LkCFhicy",
        "PtbiH6DFhic2"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx5mePashicQ"
      },
      "source": [
        "# Homework 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2xmJRh-hicS"
      },
      "source": [
        "This homework should be submitted as one notebook. When completed, please rename the notebook before sending as follows: *hw1_\\<lastname\\>_\\<firstname\\>.ipynb* (example: *hw1_Bond_James.ipynb*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehl3lr_zhicT"
      },
      "source": [
        "# Part 1: Differentiation (2.0 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgMWh8UdhicT"
      },
      "source": [
        "Since it easy to google every task please please please try to understand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. A good way to derive solutions for these tasks is to derive it for single elements and then generalize to the resulting matrix/vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVpcucu0hicU"
      },
      "source": [
        "Useful links: \n",
        "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
        "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
        "[3](http://cal.cs.illinois.edu/~johannes/research/matrix%20calculus.pdf)\n",
        "[4](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVzLyMljhicU"
      },
      "source": [
        "## ex. 1 *[0.5 pts]*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKBW5oy-hicV"
      },
      "source": [
        "Scalar w.r.t. vector:\n",
        "$$  \n",
        "y = c^Tx,  \\quad x \\in \\mathbb{R}^N \n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHeX6UcxhicV"
      },
      "source": [
        "__Note:__  $c^Tx = \\langle c, x \\rangle = c_1x_1 + c_2x_2 + \\dots + c_nx_n$\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} = \\frac{d(c^Tx)}{dx} = [\\frac{d(c^Tx)}{dx_1}, \\frac{d(c^Tx)}{dx_2}, \\dots, \\frac{d(c^Tx)}{dx_n}]^T = [c_1, c_2, \\dots, c_n]^T = c\n",
        "$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCknRqHHhicV"
      },
      "source": [
        "## ex. 2 *[0.5 pts]*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YHqcZ2xhicW"
      },
      "source": [
        "Vector w.r.t. vector:\n",
        "$$ y = \\sum_{j=1}^{N} cx^T \\quad c \\in \\mathbb{R}^{M} ,x \\in \\mathbb{R}^{N}, cx^T \\in \\mathbb{R}^{M \\times N} $$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThCBOmNChicW"
      },
      "source": [
        "__Note:__ \n",
        "$$(cx^T)_{ij} = c_ix_j$$\n",
        "$$S = x_1 + x_2 + \\dots + x_N$$\n",
        "$$y = [Sc_1, Sc_2, \\dots, Sc_M]^T \\in \\mathbb{R}^M$$\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} = \\mathbf{J}_y(x) \\in \\mathbb{R}^{N \\times M}\n",
        "$$\n",
        "\n",
        "$$\n",
        "J_y(x)_{ij} = \\frac{dy_i}{dx_j} = \\frac{dSc_i}{dx_j} = \\frac{d(c_ix_1 + c_ix_2 + \\dots + c_ix_j + \\dots + c_ix_N)}{dx_j} = c_i\n",
        "$$\n",
        "\n",
        "Then:\n",
        "$$\n",
        "\\frac{dy}{dx} = \\mathbf{J}_y(x) = \n",
        "\\begin{bmatrix}\n",
        "c_1 &  c_1 \\cdots & c_1 \\\\\n",
        "c_2 &  c_2 \\cdots & c_2 \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "c_M & c_M \\cdots & c_M\n",
        "\\end{bmatrix} \n",
        "\\in \\mathbb{R}^{N \\times M}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAVlLp-shicW"
      },
      "source": [
        "## ex. 3 *[0.5 pts]*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg5I_RCChicX"
      },
      "source": [
        "Vector w.r.t. vector:\n",
        "$$  \n",
        "y = x x^T x , x \\in \\mathbb{R}^{N}\n",
        "$$\n",
        "\n",
        "__Note__: $y = x x^Tx \\in \\mathbb{R}^{N}$\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} = \\frac{d(xx^Tx)}{dx} = \\mathbf{J}_y(x) \\in \\mathbb{R}^{N \\times N}\n",
        "$$\n",
        "\n",
        "$$\n",
        "J_y(x)_{ij} = \\frac{d(xx^Tx)_i}{dx_j}\n",
        "$$\n",
        "\n",
        "Let's define $i$-th row of matrix $xx^T$ as $(xx^T)_{(i)}$ then :\n",
        "\n",
        "$$\n",
        "\\frac{d(xx^Tx)_i}{dx_j} = \\frac{d \\langle (xx^T)_{(i)}, x \\rangle}{dx_j} = \\frac{\\sum_{k=1}^{N} x_ix_k^{2}}{dx_j}\n",
        "$$\n",
        "\n",
        "If $i \\neq j:$\n",
        "\n",
        "$$\n",
        "\\frac{\\sum_{k=1}^{N} x_ix_k^{2}}{dx_j} = 2x_ix_j\n",
        "$$\n",
        "\n",
        "Else if $i = j$:\n",
        "\n",
        "$$\n",
        "\\frac{\\sum_{k=1}^{N} x_ix_k^{2}}{dx_j} = \\frac{\\sum_{k=1}^{N} x_jx_k^{2}}{dx_j} = 2x_j^2 + \\sum_{k=1}^{N}x_k^2 = 2x_i^2 + \\sum_{k=1}^{N}x_k^2 = 2x_ix_j + \\sum_{k=1}^{N}x_ix_j \n",
        "$$\n",
        "\n",
        "We can noticed that each cell of Jacobian matrix consists $2x_ix_j$ part. We can define it as $2xx^T$ in general. When $i = j$ we also have addition equal to sum of $x_ix_j$. We can define this part as $(x^Tx)I$.\n",
        "\n",
        "In general Jacobian function will be represented as:\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} = \\frac{d(xx^Tx)}{dx} = 2xx^T + (x^Tx)I \\in \\mathbb{R}^{N \\times N}\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdffMT03hicX"
      },
      "source": [
        "## ex. 4 *[0.5 pts]*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USrZeXCVhicX"
      },
      "source": [
        "Derivatives for the parameters of the Dense layer:\n",
        "\n",
        "***Given :***  $$Y = XW, Y \\in \\mathbb{R}^{N \\times OUT}, X \\in \\mathbb{R}^{N \\times IN}, W \\in \\mathbb{R}^{IN \\times OUT} $$ \n",
        "\n",
        "The derivative of the hypothetic loss function w.r.t. to $Y$ is known: $\\Delta Y  \\in \\mathbb{R}^{N \\times OUT}$\n",
        "\n",
        "***Task :*** Please, derive the gradients of the loss w.r.t the weight matrix $W$: $\\Delta W  \\in \\mathbb{R}^{IN \\times OUT}$. Use the chain rule. First, please, derive each element of the $\\Delta W$, then generalize to the matrix form.\n",
        " \n",
        "Useful link: http://cs231n.stanford.edu/vecDerivs.pdf\n",
        "\n",
        "__Solution:__\n",
        "\n",
        "<!-- Let's define $L$ as loss function.\n",
        "\n",
        "$$\n",
        "\\frac{dL}{dY} = \\Delta Y \\in \\mathbb{R}^{N \\times OUT} \\\\\n",
        "\\frac{dL}{dX} = (\\frac{dY}{dX})^T \\frac{dL}{dY} = W^T \\frac{dL}{dY} \\in \\mathbb{R}^{N \\times IN} \\\\\n",
        "\\frac{dL}{dW} = (\\frac{dY}{dW})^T \\frac{dL}{dY} = X^T \\frac{dL}{dY} \\in \\mathbb{R}^{IN \\times OUT}\n",
        "$$\n",
        "\n",
        "\n",
        "Let's derive each element of the $\\Delta W$:\n",
        "\n",
        "$$\n",
        "\\Delta W_{ij} = \\frac{\\partial L}{\\partial W_{ij} } = \\frac{\\partial L}{\\partial Y} \\frac{\\partial Y}{\\partial W_{ij}} \n",
        "$$ -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suSo9dOlhicY"
      },
      "source": [
        "# Part 2: Modules (6.0 pts)\n",
        "\n",
        "In this part you need to implement the modules of your neural network in NumPy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmq3h1SJhicY"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in1jFhCHhicY"
      },
      "source": [
        "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDkr4zlHhicZ"
      },
      "source": [
        "class Module(object):\n",
        "    \"\"\"\n",
        "    Basically, you can think of a module as of a something (black box) \n",
        "    which can process `input` data and produce `ouput` data.\n",
        "    This is like applying a function which is called `forward`: \n",
        "        \n",
        "        output = module.forward(input)\n",
        "    \n",
        "    The module should be able to perform a backward pass: to differentiate the `forward` function. \n",
        "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
        "    The latter implies there is a gradient from previous step of a chain rule. \n",
        "    \n",
        "        gradInput = module.backward(input, gradOutput)\n",
        "    \"\"\"\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.training = True\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes an input object, and computes the corresponding output of the module.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input)\n",
        "\n",
        "    def backward(self,input, gradOutput):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the module, with respect to the given input.\n",
        "        \n",
        "        This includes \n",
        "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
        "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
        "        \"\"\"\n",
        "        self.updateGradInput(input, gradOutput)\n",
        "        self.accGradParameters(input, gradOutput)\n",
        "        return self.gradInput\n",
        "    \n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Computes the output using the current parameter set of the class and input.\n",
        "        This function returns the result which is stored in the `output` field.\n",
        "        \n",
        "        Make sure to both store the data in `output` field and return it. \n",
        "        \"\"\"\n",
        "        \n",
        "        # The easiest case:\n",
        "            \n",
        "        # self.output = input \n",
        "        # return self.output\n",
        "        \n",
        "        pass\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own input. \n",
        "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
        "        \n",
        "        The shape of `gradInput` is always the same as the shape of `input`.\n",
        "        \n",
        "        Make sure to both store the gradients in `gradInput` field and return it.\n",
        "        \"\"\"\n",
        "        \n",
        "        # The easiest case:\n",
        "        \n",
        "        # self.gradInput = gradOutput \n",
        "        # return self.gradInput\n",
        "        \n",
        "        pass   \n",
        "    \n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own parameters.\n",
        "        No need to override if module has no parameters (e.g. ReLU).\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def zeroGradParameters(self): \n",
        "        \"\"\"\n",
        "        Zeroes `gradParams` variable if the module has params.\n",
        "        \"\"\"\n",
        "        pass\n",
        "        \n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with its parameters. \n",
        "        If the module does not have parameters return empty list. \n",
        "        \"\"\"\n",
        "        return []\n",
        "        \n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with gradients with respect to its parameters. \n",
        "        If the module does not have parameters return empty list. \n",
        "        \"\"\"\n",
        "        return []\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Sets training mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "    \n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Sets evaluation mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "    \n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want \n",
        "        to have readable description. \n",
        "        \"\"\"\n",
        "        return \"Module\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhsbcwjyhica"
      },
      "source": [
        "# Sequential container *[0.5 pts]*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1q7LFsbhicb"
      },
      "source": [
        "**Define** a forward and backward pass procedures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVkQRfVQ5GnF",
        "outputId": "c6b0ef88-7a53-4e90-9feb-7cd6ec579ff1"
      },
      "source": [
        "import torch\n",
        "a = torch.randn(3, 4, 5)\n",
        "print(a[0, 0, 0].item())\n",
        "b = a\n",
        "\n",
        "a[0, 0, 0] = -100\n",
        "\n",
        "print(a[0, 0, 0], b[0, 0, 0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.9098888635635376\n",
            "tensor(-100.) tensor(-100.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IIpVLJEhicb"
      },
      "source": [
        "class Sequential(Module):\n",
        "    \"\"\"\n",
        "         This class implements a container, which processes `input` data sequentially. \n",
        "         \n",
        "         `input` is processed by each module (layer) in self.modules consecutively.\n",
        "         The resulting array is called `output`. \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__ (self):\n",
        "        super(Sequential, self).__init__()\n",
        "        self.modules = []\n",
        "   \n",
        "    def add(self, module):\n",
        "        \"\"\"\n",
        "        Adds a module to the container.\n",
        "        \"\"\"\n",
        "        self.modules.append(module)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Basic workflow of FORWARD PASS:\n",
        "        \n",
        "            y_0    = module[0].forward(input)\n",
        "            y_1    = module[1].forward(y_0)\n",
        "            ...\n",
        "            output = module[n-1].forward(y_{n-2})   \n",
        "        \n",
        "            \n",
        "        Just write a little loop. \n",
        "        \"\"\"\n",
        "\n",
        "        # Your code goes here. ################################################\n",
        "        for module_ in self.modules:\n",
        "            input = module_.forward(input)\n",
        "        self.output = input\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Workflow of BACKWARD PASS:\n",
        "            \n",
        "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
        "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
        "            ...\n",
        "            g_1 = module[1].backward(y_0, g_2)   \n",
        "            gradInput = module[0].backward(input, g_1)   \n",
        "             \n",
        "             \n",
        "        !!!\n",
        "                \n",
        "        To each module you need to provide the input, module saw while forward pass, \n",
        "        it is used while computing gradients. \n",
        "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass) \n",
        "        and NOT `input` to this Sequential module. \n",
        "        To each module you need to provide the input, module saw while forward pass, it is used while computing gradients. Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass) and NOT `input` to this Sequential module.\n",
        "        !!!\n",
        "        \n",
        "        \"\"\"\n",
        "        # Your code goes here. ################################################\n",
        "        cur_module_grad = gradOutput.copy()\n",
        "        for i in range(1, len(self.modules)):\n",
        "            cur_module = self.modules[-i]\n",
        "            prev_module = self.modules[-i - 1]\n",
        "            y_input = prev_module.output\n",
        "            # [prev_module] -> y_input -> [cur_module] ->\n",
        "            #                                          <-----cur_module_grad--------\n",
        "            cur_module_grad = cur_module.backward(y_input, cur_module_grad)\n",
        "        self.gradInput = self.modules[0].backward(input, cur_module_grad)\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def zeroGradParameters(self): \n",
        "        for module in self.modules:\n",
        "            module.zeroGradParameters()\n",
        "    \n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getParameters() for x in self.modules]\n",
        "    \n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all gradients w.r.t parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getGradParameters() for x in self.modules]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
        "        return string\n",
        "    \n",
        "    def __getitem__(self,x):\n",
        "        return self.modules.__getitem__(x)\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "        for module in self.modules:\n",
        "            module.train()\n",
        "    \n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "        for module in self.modules:\n",
        "            module.evaluate()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWUcwOAWhicd"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ettUDM2hicd"
      },
      "source": [
        "You need to implement everything in this part and make sure the code passes all tests. Read all the comments thoughtfully to ease the pain. Please try not to change the prototypes.\n",
        "\n",
        "Do not forget, that each module should return **AND** store `output` and `gradInput`.\n",
        "\n",
        "The assumption is that `module.backward` is always executed after `module.forward`,\n",
        "so `output` is stored, this would be useful for `SoftMax`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pOO1DRQhice"
      },
      "source": [
        "### Tech note\n",
        "\n",
        "* It is **strongly suggested to use vectorized operations on numpy arrays!** Avoid `for` loops whenever possible. It's very inefficient and might make your networks in Parts 3 and 4 training forever.\n",
        "\n",
        "* Prefer using `np.multiply`, `np.add`, `np.divide`, `np.subtract` instead of `*`,`+`,`/`,`-` for better memory handling. (e.g., `np.add(b,c,out = a)` instead of `a = b + c`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMnXUOC8hice"
      },
      "source": [
        "## 1. Linear transform layer  *[0.5 pts]*\n",
        "Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n",
        "- input:   **`batch_size x n_feats1`**\n",
        "- output: **`batch_size x n_feats2`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_wkprVVEB3V"
      },
      "source": [
        "??np.add"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNx9MQBbhicf"
      },
      "source": [
        "class Linear(Module):\n",
        "    \"\"\"\n",
        "    A module which applies a linear transformation \n",
        "    A common name is fully-connected layer, InnerProductLayer in caffe. \n",
        "    \n",
        "    The module should work with 2D input of shape (n_samples, n_feature).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super(Linear, self).__init__()\n",
        "       \n",
        "        # This is a nice initialization\n",
        "        stdv = 1./np.sqrt(n_in)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
        "        \n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "        \n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.add(np.dot(input, self.W.T), self.b)\n",
        "        \n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.dot(gradOutput, self.W)\n",
        "        \n",
        "        return self.gradInput\n",
        "    \n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        self.gradW = np.dot(gradOutput.T, input)\n",
        "        self.gradb = np.sum(gradOutput, axis=0)\n",
        "    \n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "        \n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "    \n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
        "        return q"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN1QyGApaAwh"
      },
      "source": [
        "lin = Linear(3, 10)\n",
        "t = torch.rand(25, 3)\n",
        "gradOutput = torch.rand(25, 10).numpy()\n",
        "lin.accGradParameters(t, gradOutput)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErgptJdlhicf"
      },
      "source": [
        "## 2. SoftMax *[0.5 pts]*\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n",
        "\n",
        "Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwQ4mg05hicg"
      },
      "source": [
        "class SoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(SoftMax, self).__init__()\n",
        "    \n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        np.subtract(input, np.max(input, axis=1, keepdims=True))\n",
        "        exp_input = np.exp(input)\n",
        "        self.output = np.divide(exp_input, np.sum(exp_input, axis=1).reshape(-1, 1))\n",
        "        \n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        softmax_output = self.output # [b, n]\n",
        "        batch_size = softmax_output.shape[0] # b\n",
        "        self.gradInput = np.zeros_like(gradOutput)\n",
        "        for i in range(batch_size):\n",
        "            J = -np.dot(softmax_output[i].reshape(-1, 1), softmax_output[i].reshape(-1, 1).T) + np.diag(softmax_output[i]) # n x n\n",
        "            J_T = J.T # n x n, but in our case jacobiam matrix is symmetric. I have added this line for clearness\n",
        "            grad_input = np.dot(J_T, gradOutput[i]) # n x 1, grad input for specific layer of batch\n",
        "            self.gradInput[i] = grad_input\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"SoftMax\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG1oIl99mJ_2",
        "outputId": "43592e17-4311-46ab-a71c-d6d11c026716"
      },
      "source": [
        "sm = SoftMax()\n",
        "# t = torch.rand(25, 10)\n",
        "t = np.random.rand(25, 10)\n",
        "sm.forward(t)\n",
        "sm.updateGradInput(t, t).shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERlLSC_ehicg"
      },
      "source": [
        "## 3. LogSoftMax *[0.5 pts]*\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n",
        "\n",
        "The main goal of this layer is to be used in computation of log-likelihood loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d23GhDQPd7u8",
        "outputId": "2ce193e8-7dcc-4958-e1c9-501006a31f92"
      },
      "source": [
        "x1 = np.arange(9.0).reshape((3, 3))\n",
        "x2 = np.arange(3.0)\n",
        "z = np.subtract(x1, x2)\n",
        "\n",
        "print(x1[1], z[1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3. 4. 5.] [3. 3. 3.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k97A8EWThicg"
      },
      "source": [
        "class LogSoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(LogSoftMax, self).__init__()\n",
        "    \n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        input = np.subtract(input, np.max(input, axis=1, keepdims=True))\n",
        "        self.output = np.subtract(input, np.log(np.sum(np.exp(input), axis=1, keepdims=True)))\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        lsm_output = self.output # b x n\n",
        "        exp_output = np.exp(lsm_output) # b x n\n",
        "\n",
        "        self.gradInput = np.zeros_like(gradOutput) # b x n\n",
        "        batch_size = lsm_output.shape[0] # b\n",
        "        n_feat = lsm_output.shape[1] # n\n",
        "        for i in range(batch_size):\n",
        "            J_T = np.subtract(np.eye(n_feat), exp_output[i]).T # n x n\n",
        "            self.gradInput[i] = np.dot(J_T, gradOutput[i]) # n x 1\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"LogSoftMax\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9VV5WoIHRrC",
        "outputId": "b5ed031b-e4dc-49e4-e76d-6936b3b74a53"
      },
      "source": [
        "bs = 25\n",
        "input = np.random.rand(bs, 10)\n",
        "lsm = LogSoftMax()\n",
        "lsm.forward(input)\n",
        "lsm.updateGradInput(input, input).shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2HqHtLihicg"
      },
      "source": [
        "## 4. Batch normalization *[0.5 pts]*\n",
        "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
        "where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance: \n",
        "```\n",
        "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
        "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
        "```\n",
        "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance. \n",
        "\n",
        "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling.\n",
        "\n",
        "**Hint:** to implement the gradients, you can derive yourself or refer to the original paper; [this guide](https://kevinzakka.github.io/2016/09/14/batch_normalization/) might also be useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glu6OJh2hicg"
      },
      "source": [
        "class BatchNormalization(Module):\n",
        "    EPS = 1e-3\n",
        "    def __init__(self, alpha = 0.):\n",
        "        super(BatchNormalization, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.moving_mean = None \n",
        "        self.moving_variance = None\n",
        "        \n",
        "    def updateOutput(self, input):         \n",
        "        if self.training:\n",
        "            batch_mean = np.mean(input, axis=0, keepdims=True)\n",
        "            centered_input = input - batch_mean\n",
        "            batch_variance = np.var(input, axis=0, keepdims=True)\n",
        "            \n",
        "            if self.moving_mean is None:\n",
        "                self.moving_mean = batch_mean\n",
        "            else:\n",
        "                self.moving_mean = self.moving_mean * self.alpha + batch_mean * (1 - self.alpha)\n",
        "            \n",
        "            if self.moving_variance is None:\n",
        "                self.moving_variance = batch_variance\n",
        "            else:\n",
        "                self.moving_variance = self.moving_variance * self.alpha + batch_variance * (1 - self.alpha)\n",
        "            \n",
        "            self.output = centered_input / np.sqrt(batch_variance + self.EPS)\n",
        "        else:\n",
        "            if self.moving_mean is None:\n",
        "                self.output = input\n",
        "            else:\n",
        "                self.output = (input - self.moving_mean) / np.sqrt(self.moving_variance + self.EPS)\n",
        "            \n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # https://arxiv.org/pdf/1502.03167.pdf\n",
        "        # https://kevinzakka.github.io/2016/09/14/batch_normalization/ (simplifier formula)\n",
        "        m = input.shape[0] # b x n (batch_size)\n",
        "        self.gradInput = np.zeros_like(gradOutput) # b x n\n",
        "        # gradOutput shape: b x n\n",
        "        bn_out = self.output # b x n\n",
        "        batch_mean = np.mean(bn_out, axis=0, keepdims=True) # 1 x n \n",
        "        batch_var = np.var(bn_out, axis=0, keepdims=True) # 1 x n \n",
        "        normalized_x = np.divide(bn_out - batch_mean, \\\n",
        "                                 np.sqrt(np.add(batch_var, self.EPS))) # b x n,  \\hat{x}\n",
        "        der_part_1 = np.subtract(m * gradOutput, np.sum(gradOutput, axis=0)) # b x n\n",
        "        der_part_2 = np.multiply(normalized_x, np.sum(np.multiply(gradOutput, normalized_x), axis=0)) # b x n\n",
        "        \n",
        "        self.gradInput = np.subtract(der_part_1, der_part_2)\n",
        "\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"BatchNormalization\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UUrFm0YGmC9",
        "outputId": "cd1e9fe5-9376-494a-db41-a4b98f3a443b"
      },
      "source": [
        "bs = 25\n",
        "N = 100\n",
        "\n",
        "input = np.random.rand(bs, N)\n",
        "bn = BatchNormalization()\n",
        "bn.forward(input)\n",
        "bn.updateGradInput(input, input).shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDLFKCyDhich"
      },
      "source": [
        "class ChannelwiseScaling(Module):\n",
        "    \"\"\"\n",
        "       Implements linear transform of input y = \\gamma * x + \\beta\n",
        "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, n_out):\n",
        "        super(ChannelwiseScaling, self).__init__()\n",
        "\n",
        "        stdv = 1./np.sqrt(n_out)\n",
        "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        \n",
        "        self.gradGamma = np.zeros_like(self.gamma)\n",
        "        self.gradBeta = np.zeros_like(self.beta)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = input * self.gamma + self.beta\n",
        "        return self.output\n",
        "        \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput * self.gamma\n",
        "        return self.gradInput\n",
        "    \n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
        "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
        "    \n",
        "    def zeroGradParameters(self):\n",
        "        self.gradGamma.fill(0)\n",
        "        self.gradBeta.fill(0)\n",
        "        \n",
        "    def getParameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "    \n",
        "    def getGradParameters(self):\n",
        "        return [self.gradGamma, self.gradBeta]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ChannelwiseScaling\""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcwrCKdFhich"
      },
      "source": [
        "Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG2lchInhich"
      },
      "source": [
        "## 5. Dropout *[0.5 pts]*\n",
        "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multiply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
        "\n",
        "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
        "\n",
        "While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `self.output = input`.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0V3TF5NUGD6",
        "outputId": "fcc53ca9-9cd3-447c-a84e-6b4484747750"
      },
      "source": [
        "msk = np.random.binomial(1, 0.01, size=(10, 10))\n",
        "arr = np.random.rand(10, 10)\n",
        "arr * msk"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfdrtodIhich"
      },
      "source": [
        "class Dropout(Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super(Dropout, self).__init__()\n",
        "        \n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "        \n",
        "    def updateOutput(self, input):\n",
        "        if self.training:\n",
        "            self.mask = np.random.binomial(1, self.p, size=input.shape)\n",
        "            self.output = np.divide(np.multiply(input, self.mask), 1-self.p)\n",
        "        else:\n",
        "            self.output = input\n",
        "\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.zeros_like(gradOutput)\n",
        "\n",
        "        if self.training:\n",
        "            self.gradInput = np.divide(np.multiply(self.mask, gradOutput), 1 - self.p)\n",
        "        else:\n",
        "            self.gradInput = gradOutput\n",
        "        \n",
        "        return self.gradInput \n",
        "        \n",
        "    def __repr__(self):\n",
        "        return \"Dropout\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjcENHB-X0r9",
        "outputId": "c968d85e-7066-4355-b00a-c320c47fa8a4"
      },
      "source": [
        "arr = np.random.rand(25, 100)\n",
        "drop = Dropout()\n",
        "drop.updateOutput(arr)\n",
        "drop.updateGradInput(arr, arr)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.73951398, 0.04556445, 1.22734953, ..., 1.91172053, 0.02705552,\n",
              "        0.        ],\n",
              "       [0.        , 0.63504351, 1.51213002, ..., 0.14119062, 0.30327762,\n",
              "        0.77215789],\n",
              "       [0.        , 1.17659322, 0.        , ..., 1.06671145, 0.52265556,\n",
              "        1.49029243],\n",
              "       ...,\n",
              "       [0.        , 0.43565869, 0.        , ..., 1.41894202, 0.43708202,\n",
              "        1.03174521],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 1.10454926,\n",
              "        0.        ],\n",
              "       [0.        , 0.83171451, 0.        , ..., 0.        , 0.98220117,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsbaHLtChich"
      },
      "source": [
        "# Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnn5dIDMhici"
      },
      "source": [
        "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j19wmXs4hici"
      },
      "source": [
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "         super(ReLU, self).__init__()\n",
        "    \n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.maximum(input, 0)\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.multiply(gradOutput, input > 0)\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ReLU\""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9MzOpqphici"
      },
      "source": [
        "## 6. Leaky ReLU *[0.5 pts]*\n",
        "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1Di9pbAlnEt",
        "outputId": "ea53e24c-8eb6-40a0-adfe-9ae8c2ba62bd"
      },
      "source": [
        "arr = np.random.rand(5, 5) - 0.5\n",
        "arr[arr > 0] *= 0\n",
        "arr"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        , -0.04873136, -0.3063722 , -0.43083112, -0.08889792],\n",
              "       [-0.32209456, -0.28818025, -0.46119892,  0.        , -0.4633448 ],\n",
              "       [ 0.        , -0.39591235,  0.        , -0.02404352, -0.17769015],\n",
              "       [ 0.        ,  0.        , -0.04550217,  0.        , -0.015284  ],\n",
              "       [ 0.        , -0.01594938, -0.38385054,  0.        , -0.41538746]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T-8_cExhici"
      },
      "source": [
        "class LeakyReLU(Module):\n",
        "    def __init__(self, slope = 0.03):\n",
        "        super(LeakyReLU, self).__init__()\n",
        "            \n",
        "        self.slope = slope\n",
        "        \n",
        "    def updateOutput(self, input): \n",
        "        self.output = input\n",
        "        self.output[input <= 0] *= self.slope\n",
        "        return  self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.multiply(np.add(np.multiply(input > 0, 1), \\\n",
        "                                            np.multiply(input <= 0, self.slope)), \\\n",
        "                                     gradOutput)\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"LeakyReLU\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDA5X46goAES",
        "outputId": "41eef281-3723-4962-81e3-f09ad67d53a8"
      },
      "source": [
        "lr = LeakyReLU()\n",
        "input = np.random.rand(100, 20)\n",
        "lr.updateGradInput(input, input)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.89598797, 0.30084573, 0.161122  , ..., 0.53824224, 0.05388063,\n",
              "        0.56137117],\n",
              "       [0.4949476 , 0.81428146, 0.36032543, ..., 0.99509066, 0.99411924,\n",
              "        0.8852298 ],\n",
              "       [0.94870995, 0.85616489, 0.3754942 , ..., 0.9317457 , 0.92232045,\n",
              "        0.37523679],\n",
              "       ...,\n",
              "       [0.89042362, 0.57630287, 0.97945365, ..., 0.33810224, 0.28952537,\n",
              "        0.52008757],\n",
              "       [0.11932993, 0.19221506, 0.24467241, ..., 0.62078   , 0.81305326,\n",
              "        0.51395188],\n",
              "       [0.22541461, 0.87889804, 0.10424958, ..., 0.99431681, 0.41601877,\n",
              "        0.60756546]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-Aclwmlhici"
      },
      "source": [
        "## 7. ELU *[0.5 pts]*\n",
        "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "befShsREhicj"
      },
      "source": [
        "class ELU(Module):\n",
        "    def __init__(self, alpha = 1.0):\n",
        "        super(ELU, self).__init__()\n",
        "        \n",
        "        self.alpha = alpha\n",
        "        \n",
        "    def updateOutput(self, input):\n",
        "        self.output = input\n",
        "        self.output[input <= 0] = np.multiply(self.alpha, np.subtract(np.exp(self.output[input <= 0]), 1)) \n",
        "        \n",
        "        return  self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.add(np.multiply(input > 0, 1), np.multiply(input <= 0, np.multiply(self.alpha, np.exp(input))))\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ELU\""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w03De60xppO5",
        "outputId": "39d65424-3797-4d97-921d-c6ededf8e3de"
      },
      "source": [
        "lr = ELU()\n",
        "input = np.random.rand(5, 5) - 0.5\n",
        "lr.updateOutput(input)#, input)\n",
        "lr.updateGradInput(input, input)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.70705836, 1.        , 0.78952785, 0.9729976 , 0.94632758],\n",
              "       [1.        , 1.        , 0.9867848 , 1.        , 1.        ],\n",
              "       [1.        , 1.        , 0.92878694, 0.7682671 , 0.75967383],\n",
              "       [0.72751298, 1.        , 0.67940053, 0.73477846, 1.        ],\n",
              "       [0.71787175, 0.87435748, 1.        , 1.        , 1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE0sq__Shicj"
      },
      "source": [
        "## 8. SoftPlus *[0.5 pts]*\n",
        "Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTBvsvNIhicj"
      },
      "source": [
        "import scipy.special\n",
        "\n",
        "class SoftPlus(Module):\n",
        "    def __init__(self):\n",
        "        super(SoftPlus, self).__init__()\n",
        "    \n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.log(np.add(1, np.exp(input)))\n",
        "        \n",
        "        return  self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.divide(gradOutput, \\\n",
        "                                   np.add(np.exp(-input), 1))\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"SoftPlus\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJkr7R8ttq1z",
        "outputId": "6d7eb7b4-4dc8-42f3-a58f-d54ad35ffb6c"
      },
      "source": [
        "sp = SoftPlus()\n",
        "input = np.random.rand(5, 5) - 0.5\n",
        "sp.updateOutput(input)#, input)\n",
        "sp.updateGradInput(input, input)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.16635943, -0.08125487,  0.26149944, -0.12905912,  0.2367863 ],\n",
              "       [ 0.10138769,  0.21726048, -0.08038228,  0.22477399, -0.11388997],\n",
              "       [-0.16183068,  0.0353377 , -0.1091312 , -0.09463425, -0.09525443],\n",
              "       [ 0.04376784, -0.09033997, -0.08754822, -0.0548076 ,  0.11203178],\n",
              "       [-0.11692413,  0.03528099,  0.24222493, -0.17428691, -0.00153064]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4vnlnVJhicj"
      },
      "source": [
        "# Criterions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzSXw_P8hicj"
      },
      "source": [
        "Criterions are used to score the models answers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A0HKToChick"
      },
      "source": [
        "class Criterion(object):\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the loss function \n",
        "            associated to the criterion and return the result.\n",
        "            \n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateOutput`.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input, target)\n",
        "\n",
        "    def backward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the gradients of the loss function\n",
        "            associated to the criterion and return the result. \n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateGradInput`.\n",
        "        \"\"\"\n",
        "        return self.updateGradInput(input, target)\n",
        "    \n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.gradInput   \n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want \n",
        "        to have readable description. \n",
        "        \"\"\"\n",
        "        return \"Criterion\""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNlcVhofhick"
      },
      "source": [
        "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- target: **`batch_size x n_feats`**\n",
        "- output: **scalar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWVIoWoehick"
      },
      "source": [
        "class MSECriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        super(MSECriterion, self).__init__()\n",
        "        \n",
        "    def updateOutput(self, input, target):   \n",
        "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
        "        return self.output \n",
        " \n",
        "    def updateGradInput(self, input, target):\n",
        "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSECriterion\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzAiSy7ghick"
      },
      "source": [
        "## 9. Negative LogLikelihood criterion (numerically unstable) *[0.5 pts]*\n",
        "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula, \n",
        "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n",
        "- input:   **`batch_size x n_feats`** - probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXhNEghehick"
      },
      "source": [
        "class ClassNLLCriterionUnstable(Criterion):\n",
        "    EPS = 1e-15\n",
        "    def __init__(self):\n",
        "        a = super(ClassNLLCriterionUnstable, self)\n",
        "        super(ClassNLLCriterionUnstable, self).__init__()\n",
        "        \n",
        "    def updateOutput(self, input, target): \n",
        "        # Use this trick to avoid numerical errors\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "        \n",
        "        self.output = -np.multiply(target, np.log(input_clamp)).sum() / input_clamp.shape[0]\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        # Use this trick to avoid numerical errors\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "                \n",
        "        self.gradInput = np.divide(np.divide(target, input_clamp), -input_clamp.shape[0]) # b x n\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterionUnstable\""
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikgIgimFvLy4",
        "outputId": "c3657a33-8988-4fae-dbce-64a5bb9cb3cf"
      },
      "source": [
        "bs = 25\n",
        "n_feat = 100\n",
        "input = np.random.rand(bs, n_feat)\n",
        "target = np.random.randint(0, 2, size=(bs, n_feat))\n",
        "cll_loss = ClassNLLCriterionUnstable()\n",
        "\n",
        "cll_loss.updateOutput(input, target)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49.251424179868266"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJn-vyClhicl"
      },
      "source": [
        "## 10. Negative LogLikelihood criterion (numerically stable) *[0.5 pts]*\n",
        "- input:   **`batch_size x n_feats`** - log probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n",
        "Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGKAyLdUhicl"
      },
      "source": [
        "class ClassNLLCriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        a = super(ClassNLLCriterion, self)\n",
        "        super(ClassNLLCriterion, self).__init__()\n",
        "        \n",
        "    def updateOutput(self, input, target): \n",
        "        self.output = -np.multiply(target, input).sum() / input.shape[0]\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        self.gradInput = -target / input.shape[0]\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterion\""
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZgfsMHj207C",
        "outputId": "07bdb54b-3227-48f0-e90e-eaa71a196538"
      },
      "source": [
        "bs = 25\n",
        "n_feat = 100\n",
        "input = np.random.rand(bs, n_feat)\n",
        "target = np.random.randint(0, 2, size=(bs, n_feat))\n",
        "cll_loss = ClassNLLCriterion()\n",
        "\n",
        "cll_loss.updateOutput(input, target)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-26.239109618650044"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIqGMLbohicl"
      },
      "source": [
        "# Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8u2pT2bhicl"
      },
      "source": [
        "### SGD optimizer with momentum\n",
        "- `variables` - list of lists of variables (one list per layer)\n",
        "- `gradients` - list of lists of current gradients (same structure as for `variables`, one array for each var)\n",
        "- `config` - dict with optimization parameters (`learning_rate` and `momentum`)\n",
        "- `state` - dict with optimizator state (used to save accumulated gradients)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAfoYY-Mhicm"
      },
      "source": [
        "def sgd_momentum(variables, gradients, config, state):  \n",
        "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
        "    state.setdefault('accumulated_grads', {})\n",
        "    \n",
        "    var_index = 0 \n",
        "    for current_layer_vars, current_layer_grads in zip(variables, gradients): \n",
        "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
        "            \n",
        "            old_grad = state['accumulated_grads'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "            \n",
        "            np.add(config['momentum'] * old_grad, config['learning_rate'] * current_grad, out=old_grad)\n",
        "            \n",
        "            current_var -= old_grad\n",
        "            var_index += 1     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqc8wMzxhicm"
      },
      "source": [
        "## 12. [Adam](https://arxiv.org/pdf/1412.6980.pdf) optimizer *[0.5 pts]*\n",
        "- `variables` - list of lists of variables (one list per layer)\n",
        "- `gradients` - list of lists of current gradients (same structure as for `variables`, one array for each var)\n",
        "- `config` - dict with optimization parameters (`learning_rate`, `beta1`, `beta2`, `epsilon`)\n",
        "- `state` - dict with optimizator state (used to save 1st and 2nd moment for vars)\n",
        "\n",
        "Formulas for optimizer:\n",
        "\n",
        "Current step learning rate: $$\\text{lr}_t = \\text{learning_rate} * \\frac{\\sqrt{1-\\beta_2^t}} {1-\\beta_1^t}$$\n",
        "First moment of var: $$\\mu_t = \\beta_1 * \\mu_{t-1} + (1 - \\beta_1)*g$$ \n",
        "Second moment of var: $$v_t = \\beta_2 * v_{t-1} + (1 - \\beta_2)*g*g$$\n",
        "New values of var: $$\\text{variable} = \\text{variable} - \\text{lr}_t * \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBQpkRQFhicm"
      },
      "source": [
        "def adam_optimizer(variables, gradients, config, state):  \n",
        "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
        "    state.setdefault('m', {})  # first moment vars\n",
        "    state.setdefault('v', {})  # second moment vars\n",
        "    state.setdefault('t', 0)   # timestamp\n",
        "    state['t'] += 1\n",
        "    for k in ['learning_rate', 'beta1', 'beta2', 'epsilon']:\n",
        "        assert k in config, config.keys()\n",
        "    \n",
        "    var_index = 0 \n",
        "    lr_t = config['learning_rate'] * np.sqrt(1 - config['beta2']**state['t']) / (1 - config['beta1']**state['t'])\n",
        "    for current_layer_vars, current_layer_grads in zip(variables, gradients): \n",
        "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
        "            var_first_moment = state['m'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "            var_second_moment = state['v'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "            \n",
        "            # <YOUR CODE> #######################################\n",
        "            # update `current_var_first_moment`, `var_second_moment` and `current_var` values\n",
        "            np.add(config['beta1'] * var_first_moment, (1 - config['beta1']) * current_grad, out=var_first_moment)\n",
        "            np.add(config['beta2'] * var_second_moment, \\\n",
        "                   (1-config['beta2']) * np.multiply(current_grad, current_grad), out = var_second_moment)\n",
        "            np.add(current_var, -lr_t * np.divide(var_first_moment, np.add(np.sqrt(var_second_moment), config['epsilon'])), out=current_var)\n",
        "            # #####################################\n",
        "            \n",
        "            # small checks that you've updated the state; use np.add for rewriting np.arrays values\n",
        "            assert var_first_moment is state['m'].get(var_index)\n",
        "            assert var_second_moment is state['v'].get(var_index)\n",
        "            var_index += 1\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaY8CYcnhicm"
      },
      "source": [
        "## Testing the modules\n",
        "\n",
        "Please run the following code provided for you to check the correctness of the implementations. It is strongly advised to run this code frequently while completing the homework.\n",
        "\n",
        "You don't need to modify the cells in this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CAxjHD_hicn"
      },
      "source": [
        "from test_modules import make_tester\n",
        "import unittest"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf1ppSdthicn"
      },
      "source": [
        "TestLayers = make_tester({\n",
        "    'Sequential': Sequential,\n",
        "    'Linear': Linear,\n",
        "    'SoftMax': SoftMax,\n",
        "    'LogSoftMax': LogSoftMax,\n",
        "    'BatchNormalization': BatchNormalization,\n",
        "    'ChannelwiseScaling': ChannelwiseScaling,\n",
        "    'Dropout': Dropout,\n",
        "    'ReLU': ReLU,\n",
        "    'LeakyReLU': LeakyReLU,\n",
        "    'ELU': ELU,\n",
        "    'SoftPlus': SoftPlus,\n",
        "    'ClassNLLCriterion': ClassNLLCriterion,\n",
        "    'ClassNLLCriterionUnstable': ClassNLLCriterionUnstable,\n",
        "    'adam_optimizer': adam_optimizer,\n",
        "})"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYA4YZ7shicn"
      },
      "source": [
        "suite = unittest.TestLoader().loadTestsFromTestCase(TestLayers)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23O0LKtfhicn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b33dd5-1b53-49b8-b3ee-6fc8ef889850"
      },
      "source": [
        "unittest.TextTestRunner(verbosity=2).run(suite)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_BatchNormalization (test_modules.make_tester.<locals>._tester) ... FAIL\n",
            "test_ClassNLLCriterion (test_modules.make_tester.<locals>._tester) ... ok\n",
            "test_ClassNLLCriterionUnstable (test_modules.make_tester.<locals>._tester) ... ok\n",
            "test_Dropout (test_modules.make_tester.<locals>._tester) ... FAIL\n",
            "test_ELU (test_modules.make_tester.<locals>._tester) ... FAIL\n",
            "test_LeakyReLU (test_modules.make_tester.<locals>._tester) ... FAIL\n",
            "test_Linear (test_modules.make_tester.<locals>._tester) ... ok\n",
            "test_LogSoftMax (test_modules.make_tester.<locals>._tester) ... ok\n",
            "test_Sequential_BatchNorm_ChannelwiseScaling (test_modules.make_tester.<locals>._tester) ... FAIL\n",
            "test_Sequential_Linear_LeakyReLU (test_modules.make_tester.<locals>._tester) ... ok\n",
            "test_SoftMax (test_modules.make_tester.<locals>._tester) ... ok\n",
            "test_SoftPlus (test_modules.make_tester.<locals>._tester) ... ok\n",
            "test_adam_optimizer (test_modules.make_tester.<locals>._tester) ... ok\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_BatchNormalization (test_modules.make_tester.<locals>._tester)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/test_modules.py\", line 134, in test_BatchNormalization\n",
            "    self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))\n",
            "AssertionError: False is not true\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_Dropout (test_modules.make_tester.<locals>._tester)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/test_modules.py\", line 334, in test_Dropout\n",
            "    self.assertTrue(np.allclose(layer_output, layer_input))\n",
            "AssertionError: False is not true\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_ELU (test_modules.make_tester.<locals>._tester)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/test_modules.py\", line 404, in test_ELU\n",
            "    self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))\n",
            "AssertionError: False is not true\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_LeakyReLU (test_modules.make_tester.<locals>._tester)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/test_modules.py\", line 378, in test_LeakyReLU\n",
            "    self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-3))\n",
            "AssertionError: False is not true\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_Sequential_BatchNorm_ChannelwiseScaling (test_modules.make_tester.<locals>._tester)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/test_modules.py\", line 185, in test_Sequential_BatchNorm_ChannelwiseScaling\n",
            "    self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-3))\n",
            "AssertionError: False is not true\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 13 tests in 0.418s\n",
            "\n",
            "FAILED (failures=5)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=13 errors=0 failures=5>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I91eldTihico"
      },
      "source": [
        "# Part 3: Training your first network (3.0 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4i2uZ0nhico"
      },
      "source": [
        "%matplotlib inline\n",
        "from time import time, sleep\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06-vZ9wLhico"
      },
      "source": [
        "# Toy example\n",
        "\n",
        "Use this example to debug your code, start with logistic regression and then test other layers. You do not need to change anything here. This code is provided for you to test the layers. Also it is easy to use this code in MNIST task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS4Iqjshhico"
      },
      "source": [
        "# Generate some data\n",
        "N = 500\n",
        "\n",
        "X1 = np.random.randn(N,2) + np.array([2,2])\n",
        "X2 = np.random.randn(N,2) + np.array([-2,-2])\n",
        "\n",
        "Y = np.concatenate([np.ones(N),np.zeros(N)])[:,None]\n",
        "Y = np.hstack([Y, 1-Y])\n",
        "\n",
        "X = np.vstack([X1,X2])\n",
        "plt.scatter(X[:,0], X[:,1], c=Y[:,0], edgecolors= 'none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJXgyJTfhicp"
      },
      "source": [
        "Define a **logistic regression** for debugging. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZP4eGQ3hicp"
      },
      "source": [
        "net = Sequential()\n",
        "net.add(Linear(2, 2))\n",
        "net.add(LogSoftMax())\n",
        "\n",
        "criterion = ClassNLLCriterion()\n",
        "\n",
        "print(net)\n",
        "\n",
        "# Test something like this then \n",
        "\n",
        "# net = Sequential()\n",
        "# net.add(Linear(2, 4))\n",
        "# net.add(ReLU())\n",
        "# net.add(Linear(4, 2))\n",
        "# net.add(LogSoftMax())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6f8xJ4Ehicp"
      },
      "source": [
        "Start with batch_size = 1000 to make sure every step lowers the loss, then try stochastic version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLH2iyeQhicp"
      },
      "source": [
        "# Iptimizer params\n",
        "optimizer_config = {'learning_rate' : 1e-1, 'momentum': 0.9}\n",
        "optimizer_state = {}\n",
        "\n",
        "# Looping params\n",
        "n_epoch = 20\n",
        "batch_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTwu3fN6hicp"
      },
      "source": [
        "# batch generator\n",
        "def get_batches(dataset, batch_size):\n",
        "    X, Y = dataset\n",
        "    n_samples = X.shape[0]\n",
        "        \n",
        "    # Shuffle at the start of epoch\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_idx = indices[start:end]\n",
        "    \n",
        "        yield X[batch_idx], Y[batch_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "infExH7Yhicq"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ypCU6sghicq"
      },
      "source": [
        "Basic training loop. Examine it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ39pSjPhicq"
      },
      "source": [
        "loss_history = []\n",
        "\n",
        "for i in range(n_epoch):\n",
        "    for x_batch, y_batch in get_batches((X, Y), batch_size):\n",
        "        \n",
        "        net.zeroGradParameters()\n",
        "        \n",
        "        # Forward\n",
        "        predictions = net.forward(x_batch)\n",
        "        loss = criterion.forward(predictions, y_batch)\n",
        "    \n",
        "        # Backward\n",
        "        dp = criterion.backward(predictions, y_batch)\n",
        "        net.backward(x_batch, dp)\n",
        "        \n",
        "        # Update weights\n",
        "        sgd_momentum(net.getParameters(), \n",
        "                     net.getGradParameters(), \n",
        "                     optimizer_config,\n",
        "                     optimizer_state)      \n",
        "        \n",
        "        loss_history.append(loss)\n",
        "\n",
        "    # Visualize\n",
        "    display.clear_output(wait=True)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "        \n",
        "    plt.title(\"Training loss\")\n",
        "    plt.xlabel(\"#iteration\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.plot(loss_history, 'b')\n",
        "    plt.show()\n",
        "    \n",
        "    print('Current loss: %f' % loss)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vte2peM-hicq"
      },
      "source": [
        "# Digit classification "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltPOOnPahicq"
      },
      "source": [
        "We will be using old good [MNIST](http://yann.lecun.com/exdb/mnist/) as our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1j967efhicr"
      },
      "source": [
        "import mnist\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = mnist.load_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYuPoUChhicr"
      },
      "source": [
        "One-hot encode the labels first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ647LUThicr"
      },
      "source": [
        "y_train_hot = np.eye(10)[y_train]\n",
        "y_val_hot = np.eye(10)[y_val]\n",
        "y_test_hot  = np.eye(10)[y_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRSq0gXqhicr"
      },
      "source": [
        "We need to reshape our 2D data to 1D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESvwaz_Ghicr"
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_val = X_val.reshape(X_val.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nl01DYGhics"
      },
      "source": [
        "-  *[1.0 pts]* **Compare** `ReLU`, `ELU`, `LeakyReLU`, `SoftPlus` activation functions. \n",
        "Use an architecture of your choice for the comparison (no need to pick the best optimizer parameters for now). Write your personal opinion on the activation functions, think about computation times too. Plot the loss curves from activation functions comparison on a single plot. Please find a scale (log?) when the lines are distinguishable, do not forget about naming the axes, the plot should be informative. \n",
        "\n",
        "- *[0.5 pts]* **Try** inserting `BatchNormalization` **(followed by `ChannelwiseScaling`)** between `Linear` module and activation functions. For all activation functions, plot versions with and without `BatchNormalization` on a single plot. Please find a scale (log?) when the lines are distinguishable, do not forget about naming the axes, the plot should be goodlooking.  Does `BatchNormalization` help?\n",
        "\n",
        "- *[0.5 pts]* Plot the losses for two networks: one trained by momentum_sgd, another one trained by Adam. Which one performs better?  \n",
        "\n",
        "- *[1.0 pts]* Increase the number of parameters in the network. Try inserting the Dropout layer. Compare the validation performance and the gap between test and validation performances for the two experiments (with and without Dropout). Compare the loss plots. Which one converges slower?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZHlNF0Qhics"
      },
      "source": [
        "# YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBwMD_5Chics"
      },
      "source": [
        "# Part 4: Convolutional neural networks (4.0 pts)\n",
        "\n",
        "In the previous part you needed to train a dense, or fully-connected, neural network. This part of the homework is to prepare modules for training a basic convolutional network: `Conv2D`, `Flatten`, `MaxPool2d`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n8bfcN0hics"
      },
      "source": [
        "Your task is to implement a simple framework for convolutional neural networks training. While convolutional neural networks is a subject of lecture 3, we expect that there are a lot of students who are familiar with the topic.\n",
        "\n",
        "In order to successfully complete this part, you will have to:\n",
        "\n",
        "- Implement all the layers below (`Conv2d`, `MaxPool2d`; `Flatten` is already implemented for convenience). Good implementation should pass all the tests in the subsequent cells.\n",
        "- Train a CNN that has at least one `Conv2d` layer and at least one `MaxPool2d` layer and achieves at least 97% accuracy on MNIST test set.\n",
        "\n",
        "Feel free to use the code from Part 3 for debugging or as a source of code snippets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cN5j7cUhict"
      },
      "source": [
        "# Layers for Part 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xXp3Qk7hict"
      },
      "source": [
        "## 12. Conv2d *[1.5 pts]*\n",
        "- input:   **`batch_size x in_channels x h x w`**\n",
        "- output: **`batch_size x out_channels x h x w`**\n",
        "\n",
        "You should implement a layer which works like PyTorch `Conv2d` layer with `stride=1` and zero-padding outside of image using `scipy.signal.correlate` function.\n",
        "\n",
        "Practical notes:\n",
        "- While the layer name is \"convolution\", most of neural network frameworks (including TensorFlow and PyTorch) implement operation that is called [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation#Cross-correlation_of_deterministic_signals) in signal processing theory. So **don't use** `scipy.signal.convolve` since it implements [convolution](https://en.wikipedia.org/wiki/Convolution#Discrete_convolution) in terms of signal processing.\n",
        "- It may be convenient to use `np.pad` (or `skimage.util.pad`, which is probably deprecated nowadays) for zero-padding. **Note that any impelentations with non-zero padding won't pass the tests!**\n",
        "- It's rather ok to implement convolution over 4d array using 2 nested loops: one over batch size dimension and another one over output filters dimension\n",
        "- Having troubles with understanding how to implement the layer? \n",
        " - Check out e.g. this link on how to make forward pass and to calculate all the gradients: https://towardsdatascience.com/backpropagation-in-a-convolutional-layer-24c8d64d8509\n",
        " - May the google be with you"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZX24q-Thicu"
      },
      "source": [
        "import scipy as sp\n",
        "import scipy.signal\n",
        "\n",
        "class Conv2d(Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "        super(Conv2d, self).__init__()\n",
        "        assert kernel_size % 2 == 1, kernel_size\n",
        "       \n",
        "        stdv = 1./np.sqrt(in_channels)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size = (out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size=(out_channels,))\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "        \n",
        "    def updateOutput(self, input):\n",
        "        pad_size = self.kernel_size // 2\n",
        "        # YOUR CODE ##############################\n",
        "        # 1. zero-pad the input array\n",
        "        # 2. compute convolution using scipy.signal.correlate(... , mode='valid')\n",
        "        # 3. add bias value\n",
        "        \n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        pad_size = self.kernel_size // 2\n",
        "        # YOUR CODE ##############################\n",
        "        # 1. zero-pad the gradOutput\n",
        "        # 2. compute 'self.gradInput' value using scipy.signal.correlate(... , mode='valid')\n",
        "        \n",
        "        # self.gradInput = ...\n",
        "        \n",
        "        return self.gradInput\n",
        "    \n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        pad_size = self.kernel_size // 2\n",
        "        # YOUR CODE #############\n",
        "        # 1. zero-pad the input\n",
        "        # 2. compute 'self.gradW' using scipy.signal.correlate(... , mode='valid')\n",
        "        # 3. compute 'self.gradb' - formulas like in Linear of ChannelwiseScaling layers\n",
        "        \n",
        "        # self.gradW = ...\n",
        "        # self.gradb = ...\n",
        "        pass\n",
        "    \n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "        \n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "    \n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Conv2d %d -> %d' %(s[1],s[0])\n",
        "        return q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IGqAoCHhicu"
      },
      "source": [
        "## 13. MaxPool2d *[1.0 pts]*\n",
        "- input:   **`batch_size x n_input_channels x h x w`**\n",
        "- output: **`batch_size x n_output_channels x h // kern_size x w // kern_size`**\n",
        "\n",
        "You are to implement simplified version of pytorch `MaxPool2d` layer with stride = kernel_size. Please note, that it's not a common case that stride = kernel_size: in AlexNet and ResNet kernel_size for max-pooling was set to 3, while stride was set to 2. We introduce this restriction to make implementation simplier.\n",
        "\n",
        "Practical notes:\n",
        "- During forward pass what you need to do is just to reshape the input tensor to `[n, c, h / kern_size, kern_size, w / kern_size, kern_size]`, swap two axes and take maximums over the last two dimensions. Reshape + axes swap is sometimes called space-to-batch transform.\n",
        "- During backward pass you need to place the gradients in positions of maximal values taken during the forward pass\n",
        "- In real frameworks the indices of maximums are stored in memory during the forward pass. It is cheaper than to keep the layer input in memory and recompute the maximums. \n",
        "\n",
        "**Hint:** `np.put_along_axis` might be useful during backward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIQPVbW3hicu"
      },
      "source": [
        "class MaxPool2d(Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super(MaxPool2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.gradInput = None\n",
        "                    \n",
        "    def updateOutput(self, input):\n",
        "        input_h, input_w = input.shape[-2:]\n",
        "        # your may remove these asserts and implement MaxPool2d with padding\n",
        "        assert input_h % self.kernel_size == 0  \n",
        "        assert input_w % self.kernel_size == 0\n",
        "        \n",
        "        # YOUR CODE #############################\n",
        "        # self.output = ...\n",
        "        # self.max_indices = ...\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # YOUR CODE #############################\n",
        "        # self.gradInput = ...\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        q = 'MaxPool2d, kern %d, stride %d' %(self.kernel_size, self.kernel_size)\n",
        "        return q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk-_bcsghicv"
      },
      "source": [
        "### Flatten layer\n",
        "Just reshapes inputs and gradients. It's usually used as proxy layer between Conv2d and Linear.\n",
        "\n",
        "The layer is already implemented below, you don't need to change it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0mKY62Yhicv"
      },
      "source": [
        "class Flatten(Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    \n",
        "    def updateOutput(self, input):\n",
        "        self.output = input.reshape(len(input), -1)\n",
        "        return self.output\n",
        "    \n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput.reshape(input.shape)\n",
        "        return self.gradInput\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"Flatten\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDpnV62yhicv"
      },
      "source": [
        "# Testing your layers\n",
        "\n",
        "Please run the following cells to check if the implementations are correct (all tests are passed):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aENkTFc7hicv"
      },
      "source": [
        "from test_modules import make_advanced_tester\n",
        "import unittest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE5jsR9ehicw"
      },
      "source": [
        "TestLayersAdv = make_advanced_tester({\n",
        "    'Conv2d': Conv2d,\n",
        "    'Flatten': Flatten,\n",
        "    'MaxPool2d': MaxPool2d,\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMqgKPfGhicw"
      },
      "source": [
        "suite = unittest.TestLoader().loadTestsFromTestCase(TestLayersAdv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taPv9WLahicw"
      },
      "source": [
        "unittest.TextTestRunner(verbosity=2).run(suite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll23LkCFhicy"
      },
      "source": [
        "# Training a convolutional network  *[1.5 pts]*\n",
        "\n",
        "Here you task is to train a CNN that has at least one `Conv2d` layer and `MaxPool2d` layer which achieves at least 97% accuracy on MNIST **test** set.\n",
        "\n",
        "**Hint:** Layers implemented in this homework run on CPU, and if the network is too deep, training might last forever, so we suggest to start with smaller networks. This subtask can be solved with a ConvNet having only 3 `Conv2d` layers (or even less) with a small number of channels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR-7MMb-hicy"
      },
      "source": [
        "%matplotlib inline\n",
        "from time import time, sleep\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZTEStlQhicz"
      },
      "source": [
        "import mnist\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = mnist.load_dataset()  # your dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt4PKAiIhicz"
      },
      "source": [
        "One-hot encode the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjvRnbyXhic0"
      },
      "source": [
        "y_train_hot = np.eye(10)[y_train]\n",
        "y_val_hot = np.eye(10)[y_val]\n",
        "y_test_hot  = np.eye(10)[y_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES05f2imhic0"
      },
      "source": [
        "Our grayscale images consist of a single channel, so, we need to reshape our image data to be able to feed Conv2d layers with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7VSgmE-hic1"
      },
      "source": [
        "X_train = X_train[:, None, :, :]\n",
        "X_val = X_val[:, None, :, :]\n",
        "X_test = X_test[:, None, :, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eE7Fd1Ahic2"
      },
      "source": [
        "# YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtbiH6DFhic2"
      },
      "source": [
        "### Reward\n",
        "\n",
        "Once done with the homework, check out [this link](https://www.cs.ryerson.ca/~aharley/vis/conv/) with some beautiful 3D visualization of what convnets really learn on MNIST :)\n",
        "\n",
        "You can also see [here](https://www.cs.ryerson.ca/~aharley/vis/fc/) that fully-connected networks are not so interpretable. "
      ]
    }
  ]
}